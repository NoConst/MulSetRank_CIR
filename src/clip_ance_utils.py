# -*- coding: utf-8 -*-
"""
CLIP-based ANCE (Approximate Nearest Neighbor Negative Contrastive Estimation) utilities
for hard negative sampling in Composed Image Retrieval training.

This module provides hard negative mining capabilities for CLIP-based CIR models,
where the composed query is the element-wise sum of text and image features.

Uses Hugging Face Transformers CLIP models.
"""

import os
import torch
import torch.nn.functional as F
import numpy as np
import faiss
from typing import Tuple, List, Optional, Union
from torch.utils.data import DataLoader
from tqdm import tqdm
from pathlib import Path
import logging
from transformers import CLIPModel, CLIPProcessor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CLIPHardNegativeMiner:
    """
    ANCE-style hard negative miner for CLIP-based CIR models.
    Uses FAISS for approximate nearest neighbor search.
    
    The target features are CLIP image embeddings, and queries are
    element-wise sum of reference image and text embeddings.
    """
    
    def __init__(
        self,
        embedding_dim: int = 512,  # CLIP ViT-B/32 dim, adjust for other models
        num_negatives: int = 16,
        topk_candidates: int = 100,
        refresh_interval: int = 1,
        use_gpu: bool = False,
        cache_dir: Optional[str] = None
    ):
        """
        Initialize the CLIP hard negative miner.
        
        Args:
            embedding_dim: Dimension of CLIP embeddings (512 for ViT-B/32, 768 for ViT-L/14)
            num_negatives: Number of hard negatives to sample per query
            topk_candidates: Top-k candidates from which to sample negatives
            refresh_interval: How often to refresh the ANN index (in epochs)
            use_gpu: Whether to use GPU for FAISS search
            cache_dir: Directory to cache embeddings (optional)
        """
        self.embedding_dim = embedding_dim
        self.num_negatives = num_negatives
        self.topk_candidates = topk_candidates
        self.refresh_interval = refresh_interval
        self.use_gpu = use_gpu and torch.cuda.is_available()
        self.cache_dir = Path(cache_dir) if cache_dir else None
        
        # Index and embeddings storage
        self.index = None
        self.target_embeddings = None
        self.target_names = None
        self.name_to_idx = {}
        
        # Training state
        self.last_refresh_epoch = -1
        self.is_initialized = False
        
        if self.cache_dir:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def _create_index(self, embeddings: np.ndarray) -> faiss.Index:
        """Create a FAISS index for the embeddings using inner product (cosine similarity)."""
        dim = embeddings.shape[1]
        num_embeddings = embeddings.shape[0]
        
        logger.info(f"Creating FAISS index for {num_embeddings} embeddings with dim={dim}")
        
        # Use inner product for cosine similarity (embeddings should be normalized)
        index = faiss.IndexFlatIP(dim)
        
        if self.use_gpu:
            try:
                res = faiss.StandardGpuResources()
                res.setTempMemory(128 * 1024 * 1024)  # 128MB
                index = faiss.index_cpu_to_gpu(res, 0, index)
                logger.info("Using GPU for FAISS index")
            except Exception as e:
                logger.warning(f"Failed to use GPU for FAISS: {e}. Falling back to CPU.")
                index = faiss.IndexFlatIP(dim)
        
        logger.info("Adding embeddings to FAISS index...")
        index.add(embeddings)
        logger.info(f"FAISS index built successfully with {index.ntotal} vectors")
        return index
    
    @torch.no_grad()
    def build_index(
        self,
        clip_model,
        dataset,
        device: torch.device,
        batch_size: int = 64,
        num_workers: int = 4
    ):
        """
        Build the FAISS index from target image features using CLIP.
        
        Args:
            clip_model: The CLIP model (transformers CLIPModel) for feature extraction
            dataset: Dataset in 'classic' mode containing target images
            device: torch device
            batch_size: Batch size for feature extraction
            num_workers: Number of data loading workers
        """
        logger.info("Building FAISS index for CLIP hard negative mining...")
        
        from utils import collate_fn
        
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=True,
            collate_fn=collate_fn
        )
        
        clip_model.eval()
        all_features = []
        all_names = []
        
        for names, images in tqdm(dataloader, desc="Extracting CLIP target features"):
            images = images.to(device, non_blocking=True)
            
            # Extract image features using transformers CLIP
            # images are already preprocessed, so we pass them directly
            outputs = clip_model.get_image_features(pixel_values=images)
            image_features = outputs
            
            # Normalize features for cosine similarity
            image_features = F.normalize(image_features, dim=-1)
            
            all_features.append(image_features.cpu().numpy())
            all_names.extend(names)
            
            if len(all_features) % 100 == 0:
                torch.cuda.empty_cache()
        
        logger.info("Feature extraction completed. Concatenating features...")
        
        self.target_embeddings = np.vstack(all_features).astype('float32')
        self.target_names = all_names
        self.name_to_idx = {name: idx for idx, name in enumerate(all_names)}
        
        logger.info(f"Total features shape: {self.target_embeddings.shape}")
        
        del all_features
        torch.cuda.empty_cache()
        
        logger.info("Starting FAISS index construction...")
        self.index = self._create_index(self.target_embeddings)
        self.is_initialized = True
        
        logger.info(f"Built index with {len(self.target_names)} target images")
        
        return self.target_embeddings, self.target_names
    
    def refresh_index(
        self,
        clip_model,
        dataset,
        device: torch.device,
        current_epoch: int,
        batch_size: int = 64,
        num_workers: int = 4,
        force: bool = False
    ) -> bool:
        """
        Refresh the index if needed based on the refresh interval.
        
        Returns:
            True if the index was refreshed, False otherwise
        """
        should_refresh = force or (current_epoch - self.last_refresh_epoch >= self.refresh_interval)
        
        if should_refresh:
            self.build_index(clip_model, dataset, device, batch_size, num_workers)
            self.last_refresh_epoch = current_epoch
            return True
        
        return False
    
    def mine_hard_negatives(
        self,
        query_features: torch.Tensor,
        positive_names: List[str],
        exclude_reference_names: Optional[List[str]] = None,
        return_names: bool = True
    ) -> Union[Tuple[np.ndarray, List[List[str]]], Tuple[np.ndarray, np.ndarray]]:
        """
        Mine hard negatives for a batch of composed queries.
        
        Args:
            query_features: Composed query embeddings (batch_size, dim)
                           This should be the element-wise sum of image and text features
            positive_names: List of positive target names to exclude
            exclude_reference_names: Additional names to exclude (e.g., reference images)
            return_names: If True, return names; if False, return precomputed features
            
        Returns:
            If return_names=True:
                hard_negative_indices: Indices of hard negatives (batch_size, num_negatives)
                hard_negative_names: Names of hard negatives (batch_size, num_negatives)
            If return_names=False:
                hard_negative_indices: Indices of hard negatives (batch_size, num_negatives)
                hard_negative_features: Features of hard negatives (batch_size, num_negatives, dim)
        """
        if not self.is_initialized:
            raise RuntimeError("Index not initialized. Call build_index first.")
        
        # Normalize and convert to numpy
        query_features = F.normalize(query_features, dim=-1)
        query_np = query_features.cpu().numpy().astype('float32')
        
        batch_size = query_np.shape[0]
        
        # Search for top-k candidates
        _, I = self.index.search(query_np, self.topk_candidates)
        
        # Filter out positives and sample hard negatives
        hard_negative_indices = np.zeros((batch_size, self.num_negatives), dtype=np.int64)
        
        for i in range(batch_size):
            candidates = I[i]
            
            # Get positive index to exclude
            positive_idx = self.name_to_idx.get(positive_names[i], -1)
            
            # Get reference index to exclude (if provided)
            exclude_idx = -1
            if exclude_reference_names and i < len(exclude_reference_names):
                exclude_idx = self.name_to_idx.get(exclude_reference_names[i], -1)
            
            # Filter candidates
            valid_candidates = []
            for cand in candidates:
                if cand != positive_idx and cand != exclude_idx:
                    valid_candidates.append(cand)
                if len(valid_candidates) >= self.num_negatives:
                    break
            
            # Pad if necessary
            while len(valid_candidates) < self.num_negatives:
                rand_idx = np.random.randint(0, len(self.target_names))
                if rand_idx not in valid_candidates and rand_idx != positive_idx:
                    valid_candidates.append(rand_idx)
            
            hard_negative_indices[i] = valid_candidates[:self.num_negatives]
        
        if return_names:
            # Return names instead of precomputed features for gradient flow
            hard_negative_names = []
            for i in range(batch_size):
                batch_names = [self.target_names[idx] for idx in hard_negative_indices[i]]
                hard_negative_names.append(batch_names)
            return hard_negative_indices, hard_negative_names
        else:
            # Return precomputed features (old behavior, breaks gradient)
            hard_negative_features = self.target_embeddings[hard_negative_indices]
            return hard_negative_indices, hard_negative_features
    
    def get_features_by_names(self, names: List[str]) -> np.ndarray:
        """Get precomputed features for a list of image names."""
        indices = [self.name_to_idx[name] for name in names if name in self.name_to_idx]
        if not indices:
            return np.array([])
        return self.target_embeddings[indices]
    
    def get_features_by_indices(self, indices: Union[List[int], np.ndarray]) -> np.ndarray:
        """Get precomputed features by indices."""
        return self.target_embeddings[indices]

def contrastive_in_batch_loss(query, target, temperature=0.07, normalized=False):
    """
    query: [B, D]
    target: [B, D]
    normalized: If True, skip normalization (assume inputs are already normalized)
    """
    if not normalized:
        query = F.normalize(query, dim=-1)
        target = F.normalize(target, dim=-1)
    sim = torch.matmul(query, target.T) / temperature
    labels = torch.arange(query.shape[0], dtype=torch.long, device=query.device)
    return F.cross_entropy(sim, labels)

def contrastive_loss_hard_negative(query, positive, negatives, temperature=0.07, normalized=False):
    """
    query: [B, D]
    positive: [B, D]
    negatives: [B, K, D] (K æ˜¯è´Ÿæ ·æœ¬æ•°é‡)
    normalized: If True, skip normalization (assume inputs are already normalized)
    """
    # 1. ç‰¹å¾å½’ä¸€åŒ– (L2 Normalization) - ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æ—¶å¿…é¡»
    if not normalized:
        query = F.normalize(query, dim=-1)
        positive = F.normalize(positive, dim=-1)
        negatives = F.normalize(negatives, dim=-1)

    # 2. è®¡ç®—æ­£æ ·æœ¬ç›¸ä¼¼åº¦: [B, 1]
    # ä½¿ç”¨ einsum æˆ– sum(q*p)
    pos_sim = torch.sum(query * positive, dim=-1, keepdim=True) # [B, 1]

    # 3. è®¡ç®—è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦: [B, K]
    # query: [B, 1, D], negatives: [B, K, D] -> bmm -> [B, 1, K]
    neg_sim = torch.bmm(query.unsqueeze(1), negatives.transpose(1, 2)).squeeze(1) # [B, K]

    # 4. æ‹¼æŽ¥ logits: [B, K + 1]
    # çº¦å®šç¬¬ 0 åˆ—æ°¸è¿œæ˜¯æ­£æ ·æœ¬
    logits = torch.cat([pos_sim, neg_sim], dim=1)
    
    # 5. é™¤ä»¥æ¸©åº¦ç³»æ•°
    logits /= temperature

    # 6. ç”Ÿæˆæ ‡ç­¾: ç›®æ ‡å…¨ä¸º 0 (å› ä¸ºæ­£æ ·æœ¬åœ¨ index 0)
    labels = torch.zeros(query.shape[0], dtype=torch.long, device=query.device)

    # 7. è®¡ç®—äº¤å‰ç†µ
    loss = F.cross_entropy(logits, labels)
    return loss

def compute_local_ranking_loss(query_feat, target_feat, hard_neg_feats, margin=0.05):
    """
    è®¡ç®—å±€éƒ¨ç›¸å¯¹è¾¹é™…æŸå¤±ï¼Œç¡®ä¿æ­£æ ·æœ¬ç›¸ä¼¼åº¦é«˜äºŽç¡¬è´Ÿæ ·æœ¬ï¼ˆFalse Negativesï¼‰
    
    Args:
        query_feat: [B, D] - ç»„åˆæŸ¥è¯¢ç‰¹å¾
        target_feat: [B, D] - æ ‡æ³¨çš„æ­£æ ·æœ¬å›¾åƒç‰¹å¾
        hard_neg_feats: [B, K, D] - ç–‘ä¼¼å‡è´Ÿæ ·æœ¬çš„ç¡¬è´Ÿæ ·æœ¬å¼ é‡
        margin: è¾¹é™…å€¼ï¼Œå»ºè®®å–å€¼ 0.01 ~ 0.05 ä¹‹é—´ä»¥ä¿æŠ¤ç‰¹å¾ç©ºé—´
    """
    # 1. L2 å½’ä¸€åŒ–ï¼šä¿è¯åœ¨è¶…çƒé¢ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä¸ç ´ååŽŸå§‹ç‰¹å¾åˆ†å¸ƒ
    query_feat = F.normalize(query_feat, dim=-1)
    target_feat = F.normalize(target_feat, dim=-1)
    hard_neg_feats = F.normalize(hard_neg_feats, dim=-1)

    # 2. è®¡ç®—æ­£æ ·æœ¬ç›¸ä¼¼åº¦ s(q, p): [B, 1]
    pos_sim = torch.sum(query_feat * target_feat, dim=-1, keepdim=True)

    # 3. è®¡ç®—è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦ s(q, n): [B, K]
    # ä½¿ç”¨ bmm è®¡ç®— batch å†…çš„çŸ©é˜µä¹˜æ³•: (B, 1, D) * (B, D, K) -> (B, 1, K)
    neg_sims = torch.bmm(query_feat.unsqueeze(1), hard_neg_feats.transpose(1, 2)).squeeze(1)

    # 4. è®¡ç®— Ranking Loss: max(0, neg_sim - pos_sim + margin)
    # åªæœ‰å½“ neg_sim + margin > pos_sim æ—¶æ‰äº§ç”Ÿæ¢¯åº¦ï¼Œä¸ä¼šè¿‡åº¦æŽ¨å¼€è´Ÿæ ·æœ¬
    loss = torch.clamp(neg_sims - pos_sim + margin, min=0.0)

    # è¿”å›ž Batch çš„å¹³å‡æŸå¤±
    return loss.mean()

def compute_clip_ance_loss(
    query_features: torch.Tensor,
    target_features: torch.Tensor,
    hard_negative_features: torch.Tensor,
    temperature: float = 0.07,
    hard_negative_weight: float = 1.0,
    ref_hard_negative_features: Optional[torch.Tensor] = None,
    ref_hard_negative_weight: float = 1.0
) -> torch.Tensor:
    """
    Compute ANCE-style contrastive loss for CLIP CIR.
    
    The loss combines:
    1. In-batch contrastive loss: query vs all targets in the batch
    2. Hard negative loss: query vs hard negatives from the current model
    3. (Optional) Reference hard negative loss: query vs reference image hard negatives
    
    Args:
        query_features: Composed query features (batch_size, dim)
                       Element-wise sum of reference image + text features
        target_features: Positive target features (batch_size, dim)
        hard_negative_features: Hard negative features (batch_size, num_negatives, dim)
                               Now these are freshly encoded through current model with gradients
        temperature: Temperature scaling factor
        hard_negative_weight: Weight for hard negative loss
        ref_hard_negative_features: Reference image hard negatives (batch_size, num_ref_negatives, dim)
                                   Images similar to reference but don't match text description
        ref_hard_negative_weight: Weight for reference hard negative loss
        
    Returns:
        Combined contrastive loss
    """
    device = query_features.device
    
    # Convert hard negatives to tensor if needed (backwards compatibility)
    # Now hard_negative_features should already be a tensor with gradients
    if isinstance(hard_negative_features, np.ndarray):
        hard_neg_tensor = torch.from_numpy(hard_negative_features).float().to(device)
    else:
        hard_neg_tensor = hard_negative_features
    
    # âœ… GPUä¼˜åŒ–ï¼šç»Ÿä¸€å½’ä¸€åŒ–ä¸€æ¬¡ï¼ˆé¿å…åœ¨æ¯ä¸ªlosså‡½æ•°ä¸­é‡å¤å½’ä¸€åŒ–ï¼‰
    query_features = F.normalize(query_features, dim=-1)
    target_features = F.normalize(target_features, dim=-1)
    hard_neg_tensor = F.normalize(hard_neg_tensor, dim=-1)
    
    # Compute in-batch contrastive loss (ä¼ å…¥normalized=Trueé¿å…é‡å¤å½’ä¸€åŒ–)
    loss_in_batch = contrastive_in_batch_loss(
        query_features, target_features, temperature, normalized=True
    )
        
    # Compute local ranking loss with hard negatives
    # This loss ensures positive samples are ranked higher than hard negatives
    # loss_local_ranking = compute_local_ranking_loss(query_features, target_features, hard_neg_tensor, margin=0.0)

    loss_hard_negative = contrastive_loss_hard_negative(
        query_features, target_features, hard_neg_tensor, temperature, normalized=True
    )

    # âœ… ä¼˜åŒ–ç‰ˆæœ¬1ï¼šå‘é‡åŒ–å¾ªçŽ¯ï¼ˆä¿æŒåŽŸå§‹è¯­ä¹‰ï¼Œå‡å°‘Pythonå¾ªçŽ¯å¼€é”€ï¼‰
    # æ³¨æ„ï¼šå®Œå…¨æ‰¹é‡åŒ–ä¼šæ”¹å˜lossçš„è¯­ä¹‰ï¼ˆåˆ†æ¯èŒƒå›´ä¸åŒï¼‰ï¼Œæ‰€ä»¥ä¿ç•™å¾ªçŽ¯ä½†ä¼˜åŒ–ç´¢å¼•
    batch_size, num_negatives, dim = hard_neg_tensor.shape
    
    # # ä½¿ç”¨åˆ—è¡¨æŽ¨å¯¼å’Œtorch.stackå‡å°‘å¾ªçŽ¯å¼€é”€
    # losses = []
    # for k in range(num_negatives):
    #     target_k = hard_neg_tensor[:, k, :]  # [B, D] - ç›´æŽ¥ç´¢å¼•ï¼Œé¿å…splitå’Œsqueeze
    #     loss_k = contrastive_in_batch_loss(
    #         query_features, target_k, temperature, normalized=True  # å·²å½’ä¸€åŒ–
    #     )
    #     losses.append(loss_k)
    
    # # # å †å å¹¶æ±‚å’Œï¼ˆæ¯”ç´¯åŠ æ›´é«˜æ•ˆï¼‰
    # loss_hard_in_batch = torch.stack(losses).sum()
    
    # å¤‡æ³¨ï¼šå¦‚æžœæƒ³è¦æ›´æ¿€è¿›çš„ä¼˜åŒ–ï¼ˆæ”¹å˜è®­ç»ƒè¯­ä¹‰ï¼Œå¢žåŠ è´Ÿæ ·æœ¬éš¾åº¦ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ï¼š
    # query_repeated = query_features.unsqueeze(1).repeat(1, num_negatives, 1).view(-1, dim)
    # hard_neg_flat = hard_neg_tensor.view(-1, dim)
    # loss_hard_in_batch = contrastive_in_batch_loss(query_repeated, hard_neg_flat, temperature)
    # ä½†è¿™ä¼šè®©è®­ç»ƒæ›´éš¾ï¼ˆåˆ†æ¯æ›´å¤§ï¼‰ï¼Œå¯èƒ½å½±å“æ”¶æ•›
    
    # Combined loss
    # total_loss = loss_in_batch + hard_negative_weight * loss_hard_negative + loss_hard_in_batch
    total_loss = loss_in_batch + hard_negative_weight * loss_hard_negative
    
    # ðŸ†• Add reference hard negative loss if provided
    if ref_hard_negative_features is not None:
        # Normalize reference hard negatives
        ref_hard_neg_tensor = F.normalize(ref_hard_negative_features, dim=-1)
        
        # Compute contrastive loss: query vs reference hard negatives
        # è¿™ä¸ªlossè®©æ¨¡åž‹å­¦ä¹ åˆ°ï¼šå³ä½¿referenceç›¸ä¼¼ï¼Œå¦‚æžœä¸åŒ¹é…textæè¿°ä¹Ÿä¸åº”è¯¥è¢«æ£€ç´¢
        loss_ref_hard_negative = contrastive_loss_hard_negative(
            query_features, target_features, ref_hard_neg_tensor, temperature, normalized=True
        )
        # query_repeated = query_features.unsqueeze(1).repeat(1, num_negatives, 1).view(-1, dim)
        # ref_hard_neg_flat = ref_hard_neg_tensor.view(-1, dim)
        # loss_ref_hard_in_batch = contrastive_in_batch_loss(query_repeated, ref_hard_neg_flat, temperature)

        # loss_between_hard_and_ref_hard = 0
        # for k in range(hard_neg_tensor.shape[1]):
        #     hard_neg_k = hard_neg_tensor[:, k, :]
        #     loss_between_hard_and_ref_hard += contrastive_loss_hard_negative(
        #         query_features, hard_neg_k, ref_hard_neg_tensor, temperature, normalized=True
        #     )
            
        # total_loss = total_loss + ref_hard_negative_weight * loss_ref_hard_negative + loss_ref_hard_in_batch + loss_between_hard_and_ref_hard

        # ðŸ†• è´Ÿæ ·æœ¬å±‚æ¬¡åŒ–: è®©queryç¡¬è´Ÿæ ·æœ¬æ¯”referenceç¡¬è´Ÿæ ·æœ¬æ›´æŽ¥è¿‘query
        # ä½¿ç”¨Pairwise Sigmoid Ranking: ä¸å—è´Ÿæ ·æœ¬æ•°é‡å½±å“ï¼Œæ¢¯åº¦smooth
        # 
        # ä¼˜åŠ¿ï¼š
        # 1. æ¯ä¸ªpairç‹¬ç«‹å»ºæ¨¡ï¼Œä¸å—K_hard/K_refæ•°é‡æ¯”ä¾‹å½±å“
        # 2. ä½¿ç”¨sigmoidæä¾›smoothæ¢¯åº¦ï¼Œè®­ç»ƒæ›´ç¨³å®š
        # 3. æ¦‚çŽ‡åŒ–å»ºæ¨¡ï¼ŒlossèŒƒå›´[0,1]ï¼Œæ˜“äºŽè°ƒå‚
        
        # âš¡ å®Œå…¨å‘é‡åŒ–è®¡ç®—ï¼ˆæ— Pythonå¾ªçŽ¯ï¼‰
        # è®¡ç®—query vs queryç¡¬è´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦ [B, K_hard]
        sim_query_hard = torch.bmm(
            query_features.unsqueeze(1), 
            hard_neg_tensor.transpose(1, 2)
        ).squeeze(1)  # [B, K_hard]
        
        # è®¡ç®—query vs referenceç¡¬è´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦ [B, K_ref]
        sim_query_ref_hard = torch.bmm(
            query_features.unsqueeze(1), 
            ref_hard_neg_tensor.transpose(1, 2)
        ).squeeze(1)  # [B, K_ref]
        
        # è®¡ç®—ç›¸ä¼¼åº¦å·®å¼‚ [B, K_hard, K_ref]
        # sim_diff[b,i,j] = sim(query_b, hard_neg_i) - sim(query_b, ref_hard_neg_j)
        sim_diff = sim_query_hard.unsqueeze(2) - sim_query_ref_hard.unsqueeze(1)  # [B, K_hard, K_ref]
        
        # â­ Pairwise Sigmoid Ranking
        # ä½¿ç”¨sigmoidå°†ç›¸ä¼¼åº¦å·®æ˜ å°„åˆ°æ¦‚çŽ‡ç©ºé—´
        # sigmoid(sim_diff / T) â†’ 1 è¡¨ç¤º hard_neg æ˜Žæ˜¾æ¯” ref_hard_neg ç›¸ä¼¼åº¦é«˜
        # sigmoid(sim_diff / T) â†’ 0 è¡¨ç¤º ref_hard_neg ç›¸ä¼¼åº¦æ›´é«˜ï¼ˆéœ€è¦æƒ©ç½šï¼‰
        temperature_ranking = 0.1  # æ¸©åº¦å‚æ•°ï¼šè¶Šå°sigmoidè¶Šé™¡å³­ï¼ŒåŒºåˆ†åº¦è¶Šé«˜
        
        # è®¡ç®—logitsï¼ˆæœªç»è¿‡sigmoidçš„åŽŸå§‹åˆ†æ•°ï¼‰
        logits = sim_diff / temperature_ranking  # [B, K_hard, K_ref]
        
        # ç›®æ ‡ï¼šæ‰€æœ‰pairçš„rankingæ¦‚çŽ‡éƒ½åº”è¯¥æŽ¥è¿‘1
        # å³ï¼šæ¯ä¸ªqueryç¡¬è´Ÿæ ·æœ¬éƒ½åº”è¯¥æ¯”æ‰€æœ‰refç¡¬è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦æ›´é«˜
        target_probs = torch.ones_like(logits)
        
        # âœ… ä½¿ç”¨BCEWithLogitsLossï¼ˆæ•°å€¼æ›´ç¨³å®šï¼Œå…¼å®¹AMPï¼‰
        # å†…éƒ¨ä¼šå…ˆåšsigmoidå†è®¡ç®—BCEï¼Œé¿å…æ‰‹åŠ¨sigmoidå¸¦æ¥çš„æ•°å€¼é—®é¢˜
        # æ¯”margin lossçš„ä¼˜åŠ¿ï¼š
        # - å³ä½¿æ»¡è¶³æ¡ä»¶(sim_diff > 0)ï¼Œä»æœ‰æ¢¯åº¦é©±åŠ¨è¿›ä¸€æ­¥ä¼˜åŒ–
        # - æ¢¯åº¦å¤§å°è‡ªé€‚åº”ï¼šæŽ¥è¿‘å†³ç­–è¾¹ç•Œæ—¶æ¢¯åº¦å¤§ï¼Œè¿œç¦»æ—¶æ¢¯åº¦å°
        # - å…¼å®¹æ··åˆç²¾åº¦è®­ç»ƒ(AMP autocast)
        loss_negative_ranking = F.binary_cross_entropy_with_logits(
            logits,
            target_probs,
            reduction='mean'
        )
        
        total_loss = total_loss + ref_hard_negative_weight * loss_ref_hard_negative + 0.5 * loss_negative_ranking
    
    return total_loss

def element_wise_sum(image_features: torch.Tensor, text_features: torch.Tensor) -> torch.Tensor:
    """
    Compute normalized element-wise sum of image and text features.
    This is the composed query representation for CLIP CIR.
    
    Args:
        image_features: Reference image features (B, D)
        text_features: Text query features (B, D)
        
    Returns:
        Normalized composed features (B, D)
    """
    return F.normalize(image_features + text_features, dim=-1)

